[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kelly To",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     GitHub"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Kelly To",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     GitHub\n  \n\n  \n  \nHi, I’m Kelly! I recently graduated from the University of Texas at Austin with a B.S. in Computational Biology and minors and certificates in Business, Computer Science, Statistics, and Pre-Health.\nMy passion lies in exploring the convergence of technology, business, design, and healthcare. Specifically, I am drawn to iOS development, data analysis, and human-centered design, envisioning innovative solutions at the intersection.\nJust as my professional pursuits span industries, my personal interests are equally diverse, including mechanical keyboards, interior design, and fashion. In my free time, you can find me spending time with loved ones, exploring the city’s food scene, playing video games, or reading."
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Kelly To",
    "section": "Blog",
    "text": "Blog\nClick here to check out the latest blog posts."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Kelly To",
    "section": "Education",
    "text": "Education\n The University of Texas at Austin     B.S. in Computational Biology   |   Aug 2020 - May 2024"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Kelly To",
    "section": "Experience",
    "text": "Experience\n The University of Texas at Austin    |   Research Engineering/Scientist Associate   |   June 2024 - Present\n BoBo    |   Software Development Intern   |   March 2024 - Present\n GIST    |   User Research Intern   |   Nov 2021 - June 2022"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nFrom PyTorch to tinygrad using ONNX\n\n\n\n\n\n\n\n\n\n12 September 2024\n\n\nKelly To\n\n\n\n\n\n\n\n\nPretty Table for Parameters\n\n\n\n\n\n\n\n\n\n11 September 2024\n\n\nKelly To\n\n\n\n\n\n\n\n\nFrom tinygrad to PyTorch\n\n\n\n\n\n\n\n\n\n09 September 2024\n\n\nKelly To\n\n\n\n\n\n\n\n\nFrom PyTorch to tinygrad\n\n\n\n\n\n\n\n\n\n06 September 2024\n\n\nKelly To\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html",
    "title": "Going from PyTorch to tinygrad",
    "section": "",
    "text": "This post is the first of what will be a series documenting my explorations with comparing PyTorch and tinygrad, two deep learning frameworks. For the first couple of posts, I will be using the simple model from tinygrad’s MNIST tutorial. I wrote a PyTorch version of the code to compare the two frameworks. The experiment in this post involves training the PyTorch model and transferring the resulting weights to the tinygrad model to see if the two models produce the same probabilities.\nBelow is a side-by-side comparison of the PyTorch and tinygrad code. tinygrad’s API is quite similar to PyTorch’s API, but there are some notable differences, including the use of ‘Tensor’ instead of ‘torch’.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport torch\n\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n\n\n\n\n\n\nfrom tinygrad import Device\n\nprint(Device.DEFAULT)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n        self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n        self.l3 = nn.Linear(1600, 10)\n        self.counter = 0\n\n    def forward(self, x):\n        self.counter += 1\n        x = F.max_pool2d(F.relu(self.l1(x)), (2,2))\n        x = F.max_pool2d(F.relu(self.l2(x)), (2,2))\n        x = self.l3(F.dropout(x.flatten(1), 0.5, self.training))\n        return x\n\n\n\n\nfrom tinygrad import Tensor, nn\n\nclass Model:\n  def __init__(self):\n    self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n    self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n    self.l3 = nn.Linear(1600, 10)\n    self.counter = 0\n\n  def __call__(self, x:Tensor) -&gt; Tensor:\n    self.counter += 1\n    x = self.l1(x).relu().max_pool2d((2,2))\n    x = self.l2(x).relu().max_pool2d((2,2))\n    x = self.l3(x.flatten(1).dropout(0.5))\n    return x\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\n\n# Convert tinygrad Tensors to PyTorch tensors and normalize the data\nX_train = torch.from_numpy(X_train.numpy()).float().reshape(-1, 1, 28, 28) / 255.0\nY_train = torch.from_numpy(Y_train.numpy()).long()\nX_test = torch.from_numpy(X_test.numpy()).float().reshape(-1, 1, 28, 28) / 255.0\nY_test = torch.from_numpy(Y_test.numpy()).long()\n\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\n\n# Normalize the data\nX_train = X_train / 255.0\nX_test = X_test / 255.0\n\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).float().mean()\nprint(acc.item())  # ~10% accuracy, as expected from a random model\n\n\n\n\n\n\n\nmodel = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).mean()\nprint(acc.item())  # ~10% accuracy, as expected from a random model\n\n\n\n\n\n\n\n\n\n\n\n\n\noptimizer = torch.optim.Adam(model.parameters())\nbatch_size = 128\ndef training_step():\n    model.train()  # enables dropout\n    samples = torch.randint(high=X_train.shape[0], size=(batch_size,))\n    X, Y = X_train[samples], Y_train[samples]\n    optimizer.zero_grad()\n    outputs = model(X)\n    loss = F.nll_loss(F.log_softmax(outputs, dim=1), Y)\n    loss.backward()\n    optimizer.step()\n    return loss\ntrain_losses = []\ntest_losses = []\n\ntrain_accuracies = []\ntest_accuracies = []\n\nfor step in range(7000):\n    # Calculate train loss\n    loss = training_step()  \n    train_losses.append(loss.item())\n    \n    if step % 100 == 0:\n        model.eval()  # Disables dropout for evaluation\n        \n        with torch.no_grad():\n            # Calculate train accuracy\n            train_outputs = model(X_train)\n            train_acc = (train_outputs.argmax(dim=1) == Y_train).float().mean().item()\n            train_accuracies.append(train_acc)\n\n            # Calculate test accuracy\n            test_outputs = model(X_test)\n            test_acc = (test_outputs.argmax(dim=1) == Y_test).float().mean().item()\n            test_accuracies.append(test_acc)\n\n            # Calculate test loss\n            test_loss = F.nll_loss(F.log_softmax(model(X_test), dim=1), Y_test).item()\n            test_losses.append(test_loss)\n\n        print(f\"step {step:4d}, loss {loss.item():.2f}, train acc {train_acc*100.:.2f}%, test acc {test_acc*100.:.2f}%\")\n        model.train()  # Re-enables dropout for training\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(len(train_losses)), train_losses, label='Train Loss')\nplt.plot(range(0, len(test_losses) * 100, 100), test_losses, label='Test Loss') # every 100 steps\nplt.title('PyTorch: Train and Test Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(0, len(train_accuracies) * 100, 100), train_accuracies, label='Train Accuracy')\nplt.plot(range(0, len(test_accuracies) * 100, 100), test_accuracies, label='Test Accuracy')\nplt.title('PyTorch: Train and Test Accuracy')\nplt.xlabel('Steps')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\ntorch.save(model.state_dict(), 'pytorch_final_weights.pth')\nloaded_weights = torch.load('pytorch_final_weights.pth')\n\n\n\n\n\n\n\n\n\n\nimport torch\n\n# Load pytorch weights\nloaded_weights = torch.load('pytorch_final_weights.pth')\n\n# Transfer weights to tinygrad model\nmodel.l1.weight.assign(Tensor(loaded_weights['l1.weight'].numpy()))\nmodel.l1.bias.assign(Tensor(loaded_weights['l1.bias'].numpy()))\nmodel.l2.weight.assign(Tensor(loaded_weights['l2.weight'].numpy()))\nmodel.l2.bias.assign(Tensor(loaded_weights['l2.bias'].numpy()))\nmodel.l3.weight.assign(Tensor(loaded_weights['l3.weight'].numpy()))\nmodel.l3.bias.assign(Tensor(loaded_weights['l3.bias'].numpy()))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\nmodel.eval()\nwith torch.no_grad():\n    pytorch_probs = F.softmax(model(test_image), dim=1).numpy()\nprint(\"PyTorch probabilities:\", pytorch_probs)\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\ntinygrad_probs = model(test_image).softmax().numpy()\nprint(\"tinygrad probabilities:\", tinygrad_probs)"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html#seed",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html#seed",
    "title": "Going from PyTorch to tinygrad",
    "section": "Seed",
    "text": "Seed\n\nInitialize the random number generator"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html#device-configuration",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html#device-configuration",
    "title": "Going from PyTorch to tinygrad",
    "section": "Device configuration",
    "text": "Device configuration"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html#model",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html#model",
    "title": "Going from PyTorch to tinygrad",
    "section": "Model",
    "text": "Model"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html#get-the-mnist-dataset",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html#get-the-mnist-dataset",
    "title": "Going from PyTorch to tinygrad",
    "section": "Get the MNIST dataset",
    "text": "Get the MNIST dataset"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html#use-the-model",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html#use-the-model",
    "title": "Going from PyTorch to tinygrad",
    "section": "Use the model",
    "text": "Use the model"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html#save-the-weights",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html#save-the-weights",
    "title": "Going from PyTorch to tinygrad",
    "section": "",
    "text": "torch.save(model.state_dict(), 'pytorch_final_weights.pth')\nloaded_weights = torch.load('pytorch_final_weights.pth')"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html#final-probabilities",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html#final-probabilities",
    "title": "Going from PyTorch to tinygrad",
    "section": "Final Probabilities",
    "text": "Final Probabilities\n\nWhen comparing the probabilities produced by both models, we can see that they are very similar with negligible variations that can be attributed to floating-point precision differences."
  },
  {
    "objectID": "blog/posts/1_from_pytorch_to_tinygrad/1_from_pytorch_to_tinygrad.html",
    "href": "blog/posts/1_from_pytorch_to_tinygrad/1_from_pytorch_to_tinygrad.html",
    "title": "From PyTorch to tinygrad",
    "section": "",
    "text": "This post is the first of what will be a series documenting my explorations with comparing PyTorch and tinygrad, two deep learning frameworks. For the first couple of posts, I will be using the simple model from tinygrad’s MNIST tutorial. I wrote a PyTorch version of the code to compare the two frameworks. The experiment in this post involves training the PyTorch model and transferring the resulting weights to the tinygrad model to see if the two models produce the same probabilities.\nI have the models in two files, ‘pytorch_MNIST_start.ipynb’ and ‘tinygrad_MNIST_end.ipynb’. Below is a side-by-side comparison of the PyTorch and tinygrad code. tinygrad’s API is quite similar to PyTorch’s API, but there are some notable differences, including the use of ‘Tensor’ instead of ‘torch’.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch\n\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")  \nprint(device)\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad import Device\n\nprint(Device.DEFAULT)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n        self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n        self.l3 = nn.Linear(1600, 10)\n\n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.l1(x)), (2,2))\n        x = F.max_pool2d(F.relu(self.l2(x)), (2,2))\n        return self.l3(F.dropout(x.flatten(1), 0.5, self.training))\n\n\n\n\n\n\n\nfrom tinygrad import Tensor, nn\n\nclass Model:\n  def __init__(self):\n    self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n    self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n    self.l3 = nn.Linear(1600, 10)\n\n  def __call__(self, x:Tensor) -&gt; Tensor:\n    x = self.l1(x).relu().max_pool2d((2,2))\n    x = self.l2(x).relu().max_pool2d((2,2))\n    return self.l3(x.flatten(1).dropout(0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\n\n# Convert tinygrad Tensors to PyTorch tensors \nX_train = torch.from_numpy(X_train.numpy()).float().reshape(-1, 1, 28, 28) \nY_train = torch.from_numpy(Y_train.numpy()).long()\nX_test = torch.from_numpy(X_test.numpy()).float().reshape(-1, 1, 28, 28) \nY_test = torch.from_numpy(Y_test.numpy()).long()\n\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).float().mean()\nprint(acc.item())  \n\n\n\n\n\n\n\nmodel = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).mean()\nprint(acc.item()) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noptimizer = torch.optim.Adam(model.parameters())\nbatch_size = 128\ndef training_step():\n    model.train()  # enables dropout\n    samples = torch.randint(high=X_train.shape[0], size=(batch_size,))\n    X, Y = X_train[samples], Y_train[samples]\n    optimizer.zero_grad()\n    outputs = model(X)\n    loss = F.nll_loss(F.log_softmax(outputs, dim=1), Y)\n    loss.backward()\n    optimizer.step()\n    return loss\ntrain_losses = []\ntest_losses = []\n\ntrain_accuracies = []\ntest_accuracies = []\n\nfor step in range(7000):\n    # Calculate train loss\n    loss = training_step()  \n    train_losses.append(loss.item())\n    \n    if step % 100 == 0:\n        model.eval()  # Disables dropout for evaluation\n        \n        with torch.no_grad():\n            # Calculate train accuracy\n            train_outputs = model(X_train)\n            train_acc = (train_outputs.argmax(dim=1) == Y_train).float().mean().item()\n            train_accuracies.append(train_acc)\n\n            # Calculate test accuracy\n            test_outputs = model(X_test)\n            test_acc = (test_outputs.argmax(dim=1) == Y_test).float().mean().item()\n            test_accuracies.append(test_acc)\n\n            # Calculate test loss\n            test_loss = F.nll_loss(F.log_softmax(model(X_test), dim=1), Y_test).item()\n            test_losses.append(test_loss)\n\n        print(f\"step {step:4d}, loss {loss.item():.2f}, train acc {train_acc*100.:.2f}%, test acc {test_acc*100.:.2f}%\")\n        model.train()  # Re-enables dropout for training\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(len(train_losses)), train_losses, label='Train Loss')\nplt.plot(range(0, len(test_losses) * 100, 100), test_losses, label='Test Loss') # every 100 steps\nplt.title('PyTorch: Train and Test Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(0, len(train_accuracies) * 100, 100), train_accuracies, label='Train Accuracy')\nplt.plot(range(0, len(test_accuracies) * 100, 100), test_accuracies, label='Test Accuracy')\nplt.title('PyTorch: Train and Test Accuracy')\nplt.xlabel('Steps')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\npytorch_weights = {\n    'l1.weight': model.l1.weight.detach().numpy(),\n    'l1.bias': model.l1.bias.detach().numpy(),\n    'l2.weight': model.l2.weight.detach().numpy(),\n    'l2.bias': model.l2.bias.detach().numpy(),\n    'l3.weight': model.l3.weight.detach().numpy(),\n    'l3.bias': model.l3.bias.detach().numpy()\n}\nnp.save('pytorch_weights.npy', pytorch_weights)\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\nloaded_weights = np.load('pytorch_weights.npy', allow_pickle=True).item()\n\nmodel.l1.weight = Tensor(loaded_weights['l1.weight'])\nmodel.l1.bias = Tensor(loaded_weights['l1.bias'])\nmodel.l2.weight = Tensor(loaded_weights['l2.weight'])\nmodel.l2.bias = Tensor(loaded_weights['l2.bias'])\nmodel.l3.weight = Tensor(loaded_weights['l3.weight'])\nmodel.l3.bias = Tensor(loaded_weights['l3.bias'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\nmodel.eval()\nwith torch.no_grad():\n    pytorch_probs = F.softmax(model(test_image), dim=1).numpy()\nprint(\"PyTorch probabilities:\", pytorch_probs)\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\ntinygrad_probs = model(test_image).softmax().numpy()\nprint(\"tinygrad probabilities:\", tinygrad_probs)"
  },
  {
    "objectID": "blog/posts/1_from_pytorch_to_tinygrad/2_from_tinygrad_to_pytorch.html",
    "href": "blog/posts/1_from_pytorch_to_tinygrad/2_from_tinygrad_to_pytorch.html",
    "title": "From tinygrad to PyTorch",
    "section": "",
    "text": "This is the second post of my series documenting my explorations with comparing PyTorch and tinygrad. Like the first post, I will be using the simple model from tinygrad’s MNIST tutorial and a PyTorch version of the model that I wrote. The experiment in this post does the previous experiment the other way around (training the tinygrad model and transferring the resulting weights to the PyTorch model to see if the two models produce the same probabilities).\nI have the models in two files, ‘tinygrad_MNIST_start.ipynb’ and ‘pytorch_MNIST_end.ipynb’. Below is a side-by-side comparison of the tinygrad and PyTorch code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad import Device\n\nprint(Device.DEFAULT)\n\n\n\n\n\n\n\n\n\n\nimport torch\n\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")  \nprint(device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad import Tensor, nn\n\nclass Model:\n  def __init__(self):\n    self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n    self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n    self.l3 = nn.Linear(1600, 10)\n\n  def __call__(self, x:Tensor) -&gt; Tensor:\n    x = self.l1(x).relu().max_pool2d((2,2))\n    x = self.l2(x).relu().max_pool2d((2,2))\n    return self.l3(x.flatten(1).dropout(0.5))\n\n\n\n\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n        self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n        self.l3 = nn.Linear(1600, 10)\n\n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.l1(x)), (2,2))\n        x = F.max_pool2d(F.relu(self.l2(x)), (2,2))\n        return self.l3(F.dropout(x.flatten(1), 0.5, self.training))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\n\n# Convert tinygrad Tensors to PyTorch tensors \nX_train = torch.from_numpy(X_train.numpy()).float().reshape(-1, 1, 28, 28) \nY_train = torch.from_numpy(Y_train.numpy()).long()\nX_test = torch.from_numpy(X_test.numpy()).float().reshape(-1, 1, 28, 28) \nY_test = torch.from_numpy(Y_test.numpy()).long()\n\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).mean()\nprint(acc.item())   \n\n\n\n\n\n\n\nmodel = Model().to(device)\nX_train = X_train.to(device)\nY_train = Y_train.to(device)\nX_test = X_test.to(device)\nY_test = Y_test.to(device)\n\nacc = (model(X_test).argmax(axis=1) == Y_test).float().mean()\nprint(acc.item())  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noptim = nn.optim.Adam(nn.state.get_parameters(model))\nbatch_size = 128\ndef step():\n    Tensor.training = True  # makes dropout work\n    samples = Tensor.randint(batch_size, high=X_train.shape[0])\n    X, Y = X_train[samples], Y_train[samples]\n    optim.zero_grad()\n    loss = model(X).sparse_categorical_crossentropy(Y).backward()\n    optim.step()\n    return loss\nfrom tinygrad import TinyJit\n\njit_step = TinyJit(step)\ntrain_losses = []\ntest_losses = []\n\ntrain_accuracies = []\ntest_accuracies = []\n\nfor step in range(7000):\n    # Calculate train loss\n    loss = jit_step()\n    train_losses.append(loss.item())\n\n    if step%100 == 0:\n        Tensor.training = False     # Disables dropout for evaluation\n\n        # Calculate train accuracy\n        #train_outputs = model(X_train)\n        #train_acc = (train_outputs.argmax(axis=1) == Y_train).mean().item()\n        #train_accuracies.append(train_acc)\n\n        # Calculate test accuracy\n        test_outputs = model(X_test)\n        test_acc = (test_outputs.argmax(axis=1) == Y_test).mean().item()\n        test_accuracies.append(test_acc)\n\n        # Calculate test loss\n        test_loss = test_outputs.sparse_categorical_crossentropy(Y_test).mean().item()\n        test_losses.append(test_loss)\n\n        #print(f\"step {step:4d}, loss {loss.item():.2f}, train acc {train_acc*100.:.2f}%, test acc {test_acc*100.:.2f}%\")\n        print(f\"step {step:4d}, loss {loss.item():.2f}, test acc {test_acc*100.:.2f}%\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(len(train_losses)), train_losses, label='Train Loss')\nplt.plot(range(0, len(test_losses) * 100, 100), test_losses, label='Test Loss') # every 100 steps\nplt.title('Tinygrad: Train and Test Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 5))\n#plt.plot(range(0, len(train_accuracies) * 100, 100), train_accuracies, label='Train Accuracy')\nplt.plot(range(0, len(test_accuracies) * 100, 100), test_accuracies, label='Test Accuracy')\nplt.title('Tinygrad: Train and Test Accuracy')\nplt.xlabel('Steps')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\ntinygrad_weights = {\n    'l1.weight': model.l1.weight.numpy(),\n    'l1.bias': model.l1.bias.numpy(),\n    'l2.weight': model.l2.weight.numpy(),\n    'l2.bias': model.l2.bias.numpy(),\n    'l3.weight': model.l3.weight.numpy(),\n    'l3.bias': model.l3.bias.numpy()\n}\nnp.save('tinygrad_weights.npy', tinygrad_weights)\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\nloaded_weights = np.load('tinygrad_weights.npy', allow_pickle=True).item()\n\nmodel.l1.weight.data = torch.tensor(loaded_weights['l1.weight']).to(device)\nmodel.l1.bias.data = torch.tensor(loaded_weights['l1.bias']).to(device)\nmodel.l2.weight.data = torch.tensor(loaded_weights['l2.weight']).to(device)\nmodel.l2.bias.data = torch.tensor(loaded_weights['l2.bias']).to(device)\nmodel.l3.weight.data = torch.tensor(loaded_weights['l3.weight']).to(device)\nmodel.l3.bias.data = torch.tensor(loaded_weights['l3.bias']).to(device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\ntinygrad_probs = model(test_image).softmax().numpy()\nprint(\"tinygrad probabilities:\", tinygrad_probs)\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\nmodel.eval()\nwith torch.no_grad():\n    pytorch_probs = F.softmax(model(test_image), dim=1).cpu().numpy()\nprint(\"PyTorch probabilities:\", pytorch_probs)"
  },
  {
    "objectID": "blog/posts/3_pretty_table_for_parameters.html",
    "href": "blog/posts/3_pretty_table_for_parameters.html",
    "title": "Pretty Table for Parameters",
    "section": "",
    "text": "This post showcases a parameter counter function that displays the number of parameters for each layer in a model. The PyTorch version is from this StackOverflow answer. The tinygrad version is an adaptation of that function that I wrote to work with tinygrad.\nI tested the functions using the files from the post From PyTorch to tinygrad. The functions were able to output the same parameter counts for both models, confirming that the models are the same. Below is a side-by-side comparison of the PyTorch and tinygrad code.\n\n\n\n\n\nPyTorch\nfrom prettytable import PrettyTable\n\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    # PyTorch uses model.named_parameters() to iterate through the parameters\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad:\n            continue\n        # PyTorch uses parameter.numel() to get the number of elements in a parameter tensor\n        params = parameter.numel()\n        table.add_row([name, params])\n        total_params += params\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\\n\")\n    return total_params\n\ncount_parameters(model)\n+-----------+------------+\n|  Modules  | Parameters |\n+-----------+------------+\n| l1.weight |    288     |\n|  l1.bias  |     32     |\n| l2.weight |   18432    |\n|  l2.bias  |     64     |\n| l3.weight |   16000    |\n|  l3.bias  |     10     |\n+-----------+------------+\nTotal Trainable Params: 34826\n\n34826\n\n\n\ntinygrad\nfrom prettytable import PrettyTable\nimport numpy as np\n\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    # tinygrad manually defines layers and layer names, then iterates through them\n    layers = [model.l1, model.l2, model.l3]\n    layer_names = ['l1', 'l2', 'l3']\n    for layer, name in zip(layers, layer_names):\n        # tinygrad uses np.prod() to calculate parameter count by multiplying the dimensions within each tensor (weight and bias)\n        weight_params = np.prod(layer.weight.shape)\n        bias_params = np.prod(layer.bias.shape)\n        table.add_row([f\"{name}.weight\", weight_params])\n        table.add_row([f\"{name}.bias\", bias_params])\n        total_params += weight_params + bias_params\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\\n\")\n    return total_params\n\ncount_parameters(model)\n+-----------+------------+\n|  Modules  | Parameters |\n+-----------+------------+\n| l1.weight |    288     |\n|  l1.bias  |     32     |\n| l2.weight |   18432    |\n|  l2.bias  |     64     |\n| l3.weight |   16000    |\n|  l3.bias  |     10     |\n+-----------+------------+\nTotal Trainable Params: 34826\n\nnp.int64(34826)"
  },
  {
    "objectID": "blog/posts/2_from_tinygrad_to_pytorch.html",
    "href": "blog/posts/2_from_tinygrad_to_pytorch.html",
    "title": "From tinygrad to PyTorch",
    "section": "",
    "text": "This is the second post of my series documenting my explorations with comparing PyTorch and tinygrad. Like the first post, From PyTorch to tinygrad, I will be using the simple model from tinygrad’s MNIST tutorial and a PyTorch version of the model that I wrote. The experiment in this post does the previous experiment the other way around (training the tinygrad model and transferring the resulting weights to the PyTorch model to see if the two models produce the same probabilities).\nI have the models in two files, ‘tinygrad_MNIST_start.ipynb’ and ‘pytorch_MNIST_end.ipynb’. Below is a side-by-side comparison of the tinygrad and PyTorch code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad import Device\n\nprint(Device.DEFAULT)\n\n\n\n\n\n\n\n\n\n\nimport torch\n\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")  \nprint(device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad import Tensor, nn\n\nclass Model:\n  def __init__(self):\n    self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n    self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n    self.l3 = nn.Linear(1600, 10)\n\n  def __call__(self, x:Tensor) -&gt; Tensor:\n    x = self.l1(x).relu().max_pool2d((2,2))\n    x = self.l2(x).relu().max_pool2d((2,2))\n    return self.l3(x.flatten(1).dropout(0.5))\n\n\n\n\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n        self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n        self.l3 = nn.Linear(1600, 10)\n\n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.l1(x)), (2,2))\n        x = F.max_pool2d(F.relu(self.l2(x)), (2,2))\n        return self.l3(F.dropout(x.flatten(1), 0.5, self.training))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\n\n# Convert tinygrad Tensors to PyTorch tensors \nX_train = torch.from_numpy(X_train.numpy()).float().reshape(-1, 1, 28, 28) \nY_train = torch.from_numpy(Y_train.numpy()).long()\nX_test = torch.from_numpy(X_test.numpy()).float().reshape(-1, 1, 28, 28) \nY_test = torch.from_numpy(Y_test.numpy()).long()\n\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).mean()\nprint(acc.item())   \n\n\n\n\n\n\n\nmodel = Model().to(device)\nX_train = X_train.to(device)\nY_train = Y_train.to(device)\nX_test = X_test.to(device)\nY_test = Y_test.to(device)\n\nacc = (model(X_test).argmax(axis=1) == Y_test).float().mean()\nprint(acc.item())  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noptim = nn.optim.Adam(nn.state.get_parameters(model))\nbatch_size = 128\ndef step():\n    Tensor.training = True  # makes dropout work\n    samples = Tensor.randint(batch_size, high=X_train.shape[0])\n    X, Y = X_train[samples], Y_train[samples]\n    optim.zero_grad()\n    loss = model(X).sparse_categorical_crossentropy(Y).backward()\n    optim.step()\n    return loss\nfrom tinygrad import TinyJit\n\njit_step = TinyJit(step)\ntrain_losses = []\ntest_losses = []\n\ntrain_accuracies = []\ntest_accuracies = []\n\nfor step in range(7000):\n    # Calculate train loss\n    loss = jit_step()\n    train_losses.append(loss.item())\n\n    if step%100 == 0:\n        Tensor.training = False     # Disables dropout for evaluation\n\n        # Calculate train accuracy\n        #train_outputs = model(X_train)\n        #train_acc = (train_outputs.argmax(axis=1) == Y_train).mean().item()\n        #train_accuracies.append(train_acc)\n\n        # Calculate test accuracy\n        test_outputs = model(X_test)\n        test_acc = (test_outputs.argmax(axis=1) == Y_test).mean().item()\n        test_accuracies.append(test_acc)\n\n        # Calculate test loss\n        test_loss = test_outputs.sparse_categorical_crossentropy(Y_test).mean().item()\n        test_losses.append(test_loss)\n\n        #print(f\"step {step:4d}, loss {loss.item():.2f}, train acc {train_acc*100.:.2f}%, test acc {test_acc*100.:.2f}%\")\n        print(f\"step {step:4d}, loss {loss.item():.2f}, test acc {test_acc*100.:.2f}%\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(len(train_losses)), train_losses, label='Train Loss')\nplt.plot(range(0, len(test_losses) * 100, 100), test_losses, label='Test Loss') # every 100 steps\nplt.title('Tinygrad: Train and Test Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 5))\n#plt.plot(range(0, len(train_accuracies) * 100, 100), train_accuracies, label='Train Accuracy')\nplt.plot(range(0, len(test_accuracies) * 100, 100), test_accuracies, label='Test Accuracy')\nplt.title('Tinygrad: Train and Test Accuracy')\nplt.xlabel('Steps')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\ntinygrad_weights = {\n    'l1.weight': model.l1.weight.numpy(),\n    'l1.bias': model.l1.bias.numpy(),\n    'l2.weight': model.l2.weight.numpy(),\n    'l2.bias': model.l2.bias.numpy(),\n    'l3.weight': model.l3.weight.numpy(),\n    'l3.bias': model.l3.bias.numpy()\n}\nnp.save('tinygrad_weights.npy', tinygrad_weights)\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\nloaded_weights = np.load('tinygrad_weights.npy', allow_pickle=True).item()\n\nmodel.l1.weight.data = torch.tensor(loaded_weights['l1.weight']).to(device)\nmodel.l1.bias.data = torch.tensor(loaded_weights['l1.bias']).to(device)\nmodel.l2.weight.data = torch.tensor(loaded_weights['l2.weight']).to(device)\nmodel.l2.bias.data = torch.tensor(loaded_weights['l2.bias']).to(device)\nmodel.l3.weight.data = torch.tensor(loaded_weights['l3.weight']).to(device)\nmodel.l3.bias.data = torch.tensor(loaded_weights['l3.bias']).to(device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\ntinygrad_probs = model(test_image).softmax().numpy()\nprint(\"tinygrad probabilities:\", tinygrad_probs)\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\nmodel.eval()\nwith torch.no_grad():\n    pytorch_probs = F.softmax(model(test_image), dim=1).cpu().numpy()\nprint(\"PyTorch probabilities:\", pytorch_probs)"
  },
  {
    "objectID": "blog/posts/1_from_pytorch_to_tinygrad.html",
    "href": "blog/posts/1_from_pytorch_to_tinygrad.html",
    "title": "From PyTorch to tinygrad",
    "section": "",
    "text": "This post is the first of what will be a series documenting my explorations with comparing PyTorch and tinygrad, two deep learning frameworks. For the first couple of posts, I will be using the simple model from tinygrad’s MNIST tutorial. I wrote a PyTorch version of the code to compare the two frameworks. The experiment in this post involves training the PyTorch model and transferring the resulting weights to the tinygrad model to see if the two models produce the same probabilities.\nI have the models in two files, ‘pytorch_MNIST_start.ipynb’ and ‘tinygrad_MNIST_end.ipynb’. Below is a side-by-side comparison of the PyTorch and tinygrad code. tinygrad’s API is quite similar to PyTorch’s API, but there are some notable differences, including the use of ‘Tensor’ instead of ‘torch’.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch\n\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")  \nprint(device)\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad import Device\n\nprint(Device.DEFAULT)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n        self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n        self.l3 = nn.Linear(1600, 10)\n\n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.l1(x)), (2,2))\n        x = F.max_pool2d(F.relu(self.l2(x)), (2,2))\n        return self.l3(F.dropout(x.flatten(1), 0.5, self.training))\n\n\n\n\n\n\n\nfrom tinygrad import Tensor, nn\n\nclass Model:\n  def __init__(self):\n    self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n    self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n    self.l3 = nn.Linear(1600, 10)\n\n  def __call__(self, x:Tensor) -&gt; Tensor:\n    x = self.l1(x).relu().max_pool2d((2,2))\n    x = self.l2(x).relu().max_pool2d((2,2))\n    return self.l3(x.flatten(1).dropout(0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\n\n# Convert tinygrad Tensors to PyTorch tensors \nX_train = torch.from_numpy(X_train.numpy()).float().reshape(-1, 1, 28, 28) \nY_train = torch.from_numpy(Y_train.numpy()).long()\nX_test = torch.from_numpy(X_test.numpy()).float().reshape(-1, 1, 28, 28) \nY_test = torch.from_numpy(Y_test.numpy()).long()\n\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).float().mean()\nprint(acc.item())  \n\n\n\n\n\n\n\nmodel = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).mean()\nprint(acc.item()) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noptimizer = torch.optim.Adam(model.parameters())\nbatch_size = 128\ndef training_step():\n    model.train()  # enables dropout\n    samples = torch.randint(high=X_train.shape[0], size=(batch_size,))\n    X, Y = X_train[samples], Y_train[samples]\n    optimizer.zero_grad()\n    outputs = model(X)\n    loss = F.nll_loss(F.log_softmax(outputs, dim=1), Y)\n    loss.backward()\n    optimizer.step()\n    return loss\ntrain_losses = []\ntest_losses = []\n\ntrain_accuracies = []\ntest_accuracies = []\n\nfor step in range(7000):\n    # Calculate train loss\n    loss = training_step()  \n    train_losses.append(loss.item())\n    \n    if step % 100 == 0:\n        model.eval()  # Disables dropout for evaluation\n        \n        with torch.no_grad():\n            # Calculate train accuracy\n            train_outputs = model(X_train)\n            train_acc = (train_outputs.argmax(dim=1) == Y_train).float().mean().item()\n            train_accuracies.append(train_acc)\n\n            # Calculate test accuracy\n            test_outputs = model(X_test)\n            test_acc = (test_outputs.argmax(dim=1) == Y_test).float().mean().item()\n            test_accuracies.append(test_acc)\n\n            # Calculate test loss\n            test_loss = F.nll_loss(F.log_softmax(model(X_test), dim=1), Y_test).item()\n            test_losses.append(test_loss)\n\n        print(f\"step {step:4d}, loss {loss.item():.2f}, train acc {train_acc*100.:.2f}%, test acc {test_acc*100.:.2f}%\")\n        model.train()  # Re-enables dropout for training\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(len(train_losses)), train_losses, label='Train Loss')\nplt.plot(range(0, len(test_losses) * 100, 100), test_losses, label='Test Loss') # every 100 steps\nplt.title('PyTorch: Train and Test Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(0, len(train_accuracies) * 100, 100), train_accuracies, label='Train Accuracy')\nplt.plot(range(0, len(test_accuracies) * 100, 100), test_accuracies, label='Test Accuracy')\nplt.title('PyTorch: Train and Test Accuracy')\nplt.xlabel('Steps')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\npytorch_weights = {\n    'l1.weight': model.l1.weight.detach().numpy(),\n    'l1.bias': model.l1.bias.detach().numpy(),\n    'l2.weight': model.l2.weight.detach().numpy(),\n    'l2.bias': model.l2.bias.detach().numpy(),\n    'l3.weight': model.l3.weight.detach().numpy(),\n    'l3.bias': model.l3.bias.detach().numpy()\n}\nnp.save('pytorch_weights.npy', pytorch_weights)\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\nloaded_weights = np.load('pytorch_weights.npy', allow_pickle=True).item()\n\nmodel.l1.weight = Tensor(loaded_weights['l1.weight'])\nmodel.l1.bias = Tensor(loaded_weights['l1.bias'])\nmodel.l2.weight = Tensor(loaded_weights['l2.weight'])\nmodel.l2.bias = Tensor(loaded_weights['l2.bias'])\nmodel.l3.weight = Tensor(loaded_weights['l3.weight'])\nmodel.l3.bias = Tensor(loaded_weights['l3.bias'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\nmodel.eval()\nwith torch.no_grad():\n    pytorch_probs = F.softmax(model(test_image), dim=1).numpy()\nprint(\"PyTorch probabilities:\", pytorch_probs)\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\ntinygrad_probs = model(test_image).softmax().numpy()\nprint(\"tinygrad probabilities:\", tinygrad_probs)"
  },
  {
    "objectID": "blog/posts/tinygrad_MNIST_onnx.html",
    "href": "blog/posts/tinygrad_MNIST_onnx.html",
    "title": "From PyTorch to tinygrad using ONNX",
    "section": "",
    "text": "This post demonstrates how to use ONNX to get tinygrad to run a PyTorch model. This is an alternative to writing a whole tinygrad version of the PyTorch model you want to port, like we did in the post From PyTorch to tinygrad.\nThe PyTorch model I’m exporting is from the PyTorch file I used in that post, ‘pytorch_MNIST_start.ipynb’. The tinygrad code is in a separate file that I made called ‘tinygrad_MNIST_onnx.ipynb’. Below is the relevant code from the two files."
  },
  {
    "objectID": "blog/posts/tinygrad_MNIST_onnx.html#count-parameters",
    "href": "blog/posts/tinygrad_MNIST_onnx.html#count-parameters",
    "title": "From PyTorch to tinygrad using ONNX",
    "section": "Count parameters",
    "text": "Count parameters\n\nI tweaked the ‘count_parameters’ function from the previous blog post, Pretty Table for Parameters, to work with ONNX models. Looking at the table, we can confirm that the ONNX model has the same number of parameters as the PyTorch model.\n\nfrom prettytable import PrettyTable\nimport numpy as np\n\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for node in model.graph.initializer:\n        param_shape = node.dims\n        num_params = np.prod(param_shape)\n        table.add_row([node.name, num_params])\n        total_params += num_params\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\\n\")\n    return total_params\n\ncount_parameters(model)\n\n+-----------+------------+\n|  Modules  | Parameters |\n+-----------+------------+\n| l1.weight |    288     |\n|  l1.bias  |     32     |\n| l2.weight |   18432    |\n|  l2.bias  |     64     |\n| l3.weight |   16000    |\n|  l3.bias  |     10     |\n+-----------+------------+\nTotal Trainable Params: 34826\n\n\n\nnp.int64(34826)"
  },
  {
    "objectID": "blog/posts/tinygrad_MNIST_onnx.html#final-probabilities",
    "href": "blog/posts/tinygrad_MNIST_onnx.html#final-probabilities",
    "title": "From PyTorch to tinygrad using ONNX",
    "section": "Final probabilities",
    "text": "Final probabilities\n\nIf you refer back to the final tinygrad probabilities in From PyTorch to tinygrad, you will see that the values below are exactly the same.\n\n# Select the first test image\ntest_image = X_test[0:1]\n\n# Run the ONNX model using run_onnx function with the test image as input\n# The model expects an input with the key \"input.1\"\nonnx_output = run_onnx({\"input.1\": test_image})\n\n# Get the output tensor (single vector of 10 values, 1 for each digit class)\noutput_tensor = list(onnx_output.values())[0]\n\n# Apply softmax and convert to numpy\ntinygrad_probs = output_tensor.softmax().numpy()\n\n# Print the resulting probabilities\nprint(\"tinygrad probabilities:\", tinygrad_probs)\n\ntinygrad probabilities: [[2.6173244e-14 4.2464655e-14 6.4881434e-08 4.5528861e-09 2.0712009e-17\n  4.9732746e-11 1.3766536e-21 9.9999988e-01 8.9217121e-13 8.5283941e-10]]"
  },
  {
    "objectID": "blog/posts/tinygrad_MNIST_onnx.html#export-pytorch-model-to-onnx-format",
    "href": "blog/posts/tinygrad_MNIST_onnx.html#export-pytorch-model-to-onnx-format",
    "title": "From PyTorch to tinygrad using ONNX",
    "section": "",
    "text": "This post demonstrates how to use ONNX to use PyTorch models in tinygrad."
  },
  {
    "objectID": "blog/posts/tinygrad_MNIST_onnx.html#export-pytorch-model-as-an-onnx-model",
    "href": "blog/posts/tinygrad_MNIST_onnx.html#export-pytorch-model-as-an-onnx-model",
    "title": "From PyTorch to tinygrad using ONNX",
    "section": "Export PyTorch model as an ONNX model",
    "text": "Export PyTorch model as an ONNX model\n\nIn my ‘pytorch_MNIST_start.ipynb’ file, I added the following code to the end of the notebook to export the model as an ONNX model in a file called ‘mnist_model.onnx’.\n\nimport torch.onnx\n\ndummy_input = torch.randn(1, 1, 28, 28)\ntorch.onnx.export(model, \n                  dummy_input, \n                  \"mnist_model.onnx\")\n\n\n\n\ntinygrad"
  },
  {
    "objectID": "blog/posts/tinygrad_MNIST_onnx.html#load-onnx-model-in-tinygrad",
    "href": "blog/posts/tinygrad_MNIST_onnx.html#load-onnx-model-in-tinygrad",
    "title": "From PyTorch to tinygrad using ONNX",
    "section": "Load ONNX model in tinygrad",
    "text": "Load ONNX model in tinygrad\n\nThis loads the ONNX model in tinygrad and creates a callable object ‘run_onnx’ that can execute the model.\n\nimport onnx\nfrom extra.onnx import get_run_onnx\n\nmodel = onnx.load(\"mnist_model.onnx\")\nrun_onnx = get_run_onnx(model)"
  },
  {
    "objectID": "blog/posts/tinygrad_MNIST_onnx.html#section",
    "href": "blog/posts/tinygrad_MNIST_onnx.html#section",
    "title": "From PyTorch to tinygrad using ONNX",
    "section": "",
    "text": "tinygrad"
  },
  {
    "objectID": "blog/posts/tinygrad_MNIST_onnx.html#get-the-mnist-dataset",
    "href": "blog/posts/tinygrad_MNIST_onnx.html#get-the-mnist-dataset",
    "title": "From PyTorch to tinygrad using ONNX",
    "section": "Get the MNIST dataset",
    "text": "Get the MNIST dataset\n\nI imported the MNIST dataset to grab an image to use to test the model.\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)"
  },
  {
    "objectID": "blog/posts/4_tinygrad_MNIST_onnx.html",
    "href": "blog/posts/4_tinygrad_MNIST_onnx.html",
    "title": "From PyTorch to tinygrad using ONNX",
    "section": "",
    "text": "This post demonstrates how to use ONNX to get tinygrad to run a PyTorch model. This is an alternative to writing a whole tinygrad version of the PyTorch model you want to port, like we did in the post From PyTorch to tinygrad.\nThe PyTorch model I’m exporting is from the PyTorch file I used in that post, ‘pytorch_MNIST_start.ipynb’. The tinygrad code is in a separate file that I made called ‘tinygrad_MNIST_onnx.ipynb’. Below is the relevant code from the two files."
  },
  {
    "objectID": "blog/posts/4_tinygrad_MNIST_onnx.html#export-pytorch-model-as-an-onnx-model",
    "href": "blog/posts/4_tinygrad_MNIST_onnx.html#export-pytorch-model-as-an-onnx-model",
    "title": "From PyTorch to tinygrad using ONNX",
    "section": "Export PyTorch model as an ONNX model",
    "text": "Export PyTorch model as an ONNX model\n\nIn my ‘pytorch_MNIST_start.ipynb’ file, I added the following code to the end of the notebook to export the model as an ONNX model in a file called ‘mnist_model.onnx’.\n\nimport torch.onnx\n\ndummy_input = torch.randn(1, 1, 28, 28)\ntorch.onnx.export(model, \n                  dummy_input, \n                  \"mnist_model.onnx\")\n\n\n\n\ntinygrad"
  },
  {
    "objectID": "blog/posts/4_tinygrad_MNIST_onnx.html#load-onnx-model-in-tinygrad",
    "href": "blog/posts/4_tinygrad_MNIST_onnx.html#load-onnx-model-in-tinygrad",
    "title": "From PyTorch to tinygrad using ONNX",
    "section": "Load ONNX model in tinygrad",
    "text": "Load ONNX model in tinygrad\n\nThis loads the ONNX model in tinygrad and creates a callable object ‘run_onnx’ that can execute the model.\n\nimport onnx\nfrom extra.onnx import get_run_onnx\n\nmodel = onnx.load(\"mnist_model.onnx\")\nrun_onnx = get_run_onnx(model)"
  },
  {
    "objectID": "blog/posts/4_tinygrad_MNIST_onnx.html#count-parameters",
    "href": "blog/posts/4_tinygrad_MNIST_onnx.html#count-parameters",
    "title": "From PyTorch to tinygrad using ONNX",
    "section": "Count parameters",
    "text": "Count parameters\n\nI tweaked the ‘count_parameters’ function from the previous blog post, Pretty Table for Parameters, to work with ONNX models. Looking at the table, we can confirm that the ONNX model has the same number of parameters as the PyTorch model.\n\nfrom prettytable import PrettyTable\nimport numpy as np\n\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    # ONNX uses model.graph.initializer to iterate through the parameters (nodes)\n    for node in model.graph.initializer:\n        # ONNX uses np.prod() to calculate parameter count by multiplying the dimensions (node.dims = parameter shape)\n        num_params = np.prod(node.dims)\n        table.add_row([node.name, num_params])\n        total_params += num_params\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\\n\")\n    return total_params\n\ncount_parameters(model)\n\n+-----------+------------+\n|  Modules  | Parameters |\n+-----------+------------+\n| l1.weight |    288     |\n|  l1.bias  |     32     |\n| l2.weight |   18432    |\n|  l2.bias  |     64     |\n| l3.weight |   16000    |\n|  l3.bias  |     10     |\n+-----------+------------+\nTotal Trainable Params: 34826\n\n\n\nnp.int64(34826)"
  },
  {
    "objectID": "blog/posts/4_tinygrad_MNIST_onnx.html#get-the-mnist-dataset",
    "href": "blog/posts/4_tinygrad_MNIST_onnx.html#get-the-mnist-dataset",
    "title": "From PyTorch to tinygrad using ONNX",
    "section": "Get the MNIST dataset",
    "text": "Get the MNIST dataset\n\nI imported the MNIST dataset to grab an image to use to test the model.\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)"
  },
  {
    "objectID": "blog/posts/4_tinygrad_MNIST_onnx.html#final-probabilities",
    "href": "blog/posts/4_tinygrad_MNIST_onnx.html#final-probabilities",
    "title": "From PyTorch to tinygrad using ONNX",
    "section": "Final probabilities",
    "text": "Final probabilities\n\nIf you refer back to the final tinygrad probabilities in From PyTorch to tinygrad, you will see that the values below are exactly the same.\n\n# Select the first test image\ntest_image = X_test[0:1]\n\n# Run the ONNX model using run_onnx function with the test image as input\n# The model expects an input with the key \"input.1\"\nonnx_output = run_onnx({\"input.1\": test_image})\n\n# Get the output tensor (single vector of 10 values, 1 for each digit class)\noutput_tensor = list(onnx_output.values())[0]\n\n# Apply softmax and convert to numpy\ntinygrad_probs = output_tensor.softmax().numpy()\n\n# Print the resulting probabilities\nprint(\"tinygrad probabilities:\", tinygrad_probs)\n\ntinygrad probabilities: [[2.6173244e-14 4.2464655e-14 6.4881434e-08 4.5528861e-09 2.0712009e-17\n  4.9732746e-11 1.3766536e-21 9.9999988e-01 8.9217121e-13 8.5283941e-10]]"
  }
]