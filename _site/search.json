[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kelly To",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     GitHub"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Kelly To",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     GitHub\n  \n\n  \n  \nHi, I’m Kelly! I recently graduated from the University of Texas at Austin with a B.S. in Computational Biology and minors and certificates in Business, Computer Science, Statistics, and Pre-Health.\nMy passion lies in exploring the convergence of technology, business, design, and healthcare. Specifically, I am drawn to iOS development, data analysis, and human-centered design, envisioning innovative solutions at the intersection.\nJust as my professional pursuits span industries, my personal interests are equally diverse, including mechanical keyboards, interior design, and fashion. In my free time, you can find me spending time with loved ones, exploring the city’s food scene, playing video games, or reading."
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Kelly To",
    "section": "Blog",
    "text": "Blog\nClick here to check out the latest blog posts."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Kelly To",
    "section": "Education",
    "text": "Education\n The University of Texas at Austin     B.S. in Computational Biology   |   Aug 2020 - May 2024"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Kelly To",
    "section": "Experience",
    "text": "Experience\n The University of Texas at Austin    |   Research Engineering/Scientist Associate   |   June 2024 - Present\n BoBo    |   Software Development Intern   |   March 2024 - Present\n GIST    |   User Research Intern   |   Nov 2021 - June 2022"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nFrom tinygrad to PyTorch\n\n\n\n\n\n\n\n\n\n07 September 2024\n\n\nKelly To\n\n\n\n\n\n\n\n\nFrom PyTorch to tinygrad\n\n\n\n\n\n\n\n\n\n06 September 2024\n\n\nKelly To\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html",
    "title": "Going from PyTorch to tinygrad",
    "section": "",
    "text": "This post is the first of what will be a series documenting my explorations with comparing PyTorch and tinygrad, two deep learning frameworks. For the first couple of posts, I will be using the simple model from tinygrad’s MNIST tutorial. I wrote a PyTorch version of the code to compare the two frameworks. The experiment in this post involves training the PyTorch model and transferring the resulting weights to the tinygrad model to see if the two models produce the same probabilities.\nBelow is a side-by-side comparison of the PyTorch and tinygrad code. tinygrad’s API is quite similar to PyTorch’s API, but there are some notable differences, including the use of ‘Tensor’ instead of ‘torch’.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport torch\n\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n\n\n\n\n\n\nfrom tinygrad import Device\n\nprint(Device.DEFAULT)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n        self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n        self.l3 = nn.Linear(1600, 10)\n        self.counter = 0\n\n    def forward(self, x):\n        self.counter += 1\n        x = F.max_pool2d(F.relu(self.l1(x)), (2,2))\n        x = F.max_pool2d(F.relu(self.l2(x)), (2,2))\n        x = self.l3(F.dropout(x.flatten(1), 0.5, self.training))\n        return x\n\n\n\n\nfrom tinygrad import Tensor, nn\n\nclass Model:\n  def __init__(self):\n    self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n    self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n    self.l3 = nn.Linear(1600, 10)\n    self.counter = 0\n\n  def __call__(self, x:Tensor) -&gt; Tensor:\n    self.counter += 1\n    x = self.l1(x).relu().max_pool2d((2,2))\n    x = self.l2(x).relu().max_pool2d((2,2))\n    x = self.l3(x.flatten(1).dropout(0.5))\n    return x\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\n\n# Convert tinygrad Tensors to PyTorch tensors and normalize the data\nX_train = torch.from_numpy(X_train.numpy()).float().reshape(-1, 1, 28, 28) / 255.0\nY_train = torch.from_numpy(Y_train.numpy()).long()\nX_test = torch.from_numpy(X_test.numpy()).float().reshape(-1, 1, 28, 28) / 255.0\nY_test = torch.from_numpy(Y_test.numpy()).long()\n\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\n\n# Normalize the data\nX_train = X_train / 255.0\nX_test = X_test / 255.0\n\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).float().mean()\nprint(acc.item())  # ~10% accuracy, as expected from a random model\n\n\n\n\n\n\n\nmodel = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).mean()\nprint(acc.item())  # ~10% accuracy, as expected from a random model\n\n\n\n\n\n\n\n\n\n\n\n\n\noptimizer = torch.optim.Adam(model.parameters())\nbatch_size = 128\ndef training_step():\n    model.train()  # enables dropout\n    samples = torch.randint(high=X_train.shape[0], size=(batch_size,))\n    X, Y = X_train[samples], Y_train[samples]\n    optimizer.zero_grad()\n    outputs = model(X)\n    loss = F.nll_loss(F.log_softmax(outputs, dim=1), Y)\n    loss.backward()\n    optimizer.step()\n    return loss\ntrain_losses = []\ntest_losses = []\n\ntrain_accuracies = []\ntest_accuracies = []\n\nfor step in range(7000):\n    # Calculate train loss\n    loss = training_step()  \n    train_losses.append(loss.item())\n    \n    if step % 100 == 0:\n        model.eval()  # Disables dropout for evaluation\n        \n        with torch.no_grad():\n            # Calculate train accuracy\n            train_outputs = model(X_train)\n            train_acc = (train_outputs.argmax(dim=1) == Y_train).float().mean().item()\n            train_accuracies.append(train_acc)\n\n            # Calculate test accuracy\n            test_outputs = model(X_test)\n            test_acc = (test_outputs.argmax(dim=1) == Y_test).float().mean().item()\n            test_accuracies.append(test_acc)\n\n            # Calculate test loss\n            test_loss = F.nll_loss(F.log_softmax(model(X_test), dim=1), Y_test).item()\n            test_losses.append(test_loss)\n\n        print(f\"step {step:4d}, loss {loss.item():.2f}, train acc {train_acc*100.:.2f}%, test acc {test_acc*100.:.2f}%\")\n        model.train()  # Re-enables dropout for training\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(len(train_losses)), train_losses, label='Train Loss')\nplt.plot(range(0, len(test_losses) * 100, 100), test_losses, label='Test Loss') # every 100 steps\nplt.title('PyTorch: Train and Test Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(0, len(train_accuracies) * 100, 100), train_accuracies, label='Train Accuracy')\nplt.plot(range(0, len(test_accuracies) * 100, 100), test_accuracies, label='Test Accuracy')\nplt.title('PyTorch: Train and Test Accuracy')\nplt.xlabel('Steps')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\ntorch.save(model.state_dict(), 'pytorch_final_weights.pth')\nloaded_weights = torch.load('pytorch_final_weights.pth')\n\n\n\n\n\n\n\n\n\n\nimport torch\n\n# Load pytorch weights\nloaded_weights = torch.load('pytorch_final_weights.pth')\n\n# Transfer weights to tinygrad model\nmodel.l1.weight.assign(Tensor(loaded_weights['l1.weight'].numpy()))\nmodel.l1.bias.assign(Tensor(loaded_weights['l1.bias'].numpy()))\nmodel.l2.weight.assign(Tensor(loaded_weights['l2.weight'].numpy()))\nmodel.l2.bias.assign(Tensor(loaded_weights['l2.bias'].numpy()))\nmodel.l3.weight.assign(Tensor(loaded_weights['l3.weight'].numpy()))\nmodel.l3.bias.assign(Tensor(loaded_weights['l3.bias'].numpy()))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\nmodel.eval()\nwith torch.no_grad():\n    pytorch_probs = F.softmax(model(test_image), dim=1).numpy()\nprint(\"PyTorch probabilities:\", pytorch_probs)\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\ntinygrad_probs = model(test_image).softmax().numpy()\nprint(\"tinygrad probabilities:\", tinygrad_probs)"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html#seed",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html#seed",
    "title": "Going from PyTorch to tinygrad",
    "section": "Seed",
    "text": "Seed\n\nInitialize the random number generator"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html#device-configuration",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html#device-configuration",
    "title": "Going from PyTorch to tinygrad",
    "section": "Device configuration",
    "text": "Device configuration"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html#model",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html#model",
    "title": "Going from PyTorch to tinygrad",
    "section": "Model",
    "text": "Model"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html#get-the-mnist-dataset",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html#get-the-mnist-dataset",
    "title": "Going from PyTorch to tinygrad",
    "section": "Get the MNIST dataset",
    "text": "Get the MNIST dataset"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html#use-the-model",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html#use-the-model",
    "title": "Going from PyTorch to tinygrad",
    "section": "Use the model",
    "text": "Use the model"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html#save-the-weights",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html#save-the-weights",
    "title": "Going from PyTorch to tinygrad",
    "section": "",
    "text": "torch.save(model.state_dict(), 'pytorch_final_weights.pth')\nloaded_weights = torch.load('pytorch_final_weights.pth')"
  },
  {
    "objectID": "blog/posts/1_port_pytorch_to_tinygrad.html#final-probabilities",
    "href": "blog/posts/1_port_pytorch_to_tinygrad.html#final-probabilities",
    "title": "Going from PyTorch to tinygrad",
    "section": "Final Probabilities",
    "text": "Final Probabilities\n\nWhen comparing the probabilities produced by both models, we can see that they are very similar with negligible variations that can be attributed to floating-point precision differences."
  },
  {
    "objectID": "blog/posts/1_from_pytorch_to_tinygrad/1_from_pytorch_to_tinygrad.html",
    "href": "blog/posts/1_from_pytorch_to_tinygrad/1_from_pytorch_to_tinygrad.html",
    "title": "From PyTorch to tinygrad",
    "section": "",
    "text": "This post is the first of what will be a series documenting my explorations with comparing PyTorch and tinygrad, two deep learning frameworks. For the first couple of posts, I will be using the simple model from tinygrad’s MNIST tutorial. I wrote a PyTorch version of the code to compare the two frameworks. The experiment in this post involves training the PyTorch model and transferring the resulting weights to the tinygrad model to see if the two models produce the same probabilities.\nI have the models in two files, ‘pytorch_MNIST_start.ipynb’ and ‘tinygrad_MNIST_end.ipynb’. Below is a side-by-side comparison of the PyTorch and tinygrad code. tinygrad’s API is quite similar to PyTorch’s API, but there are some notable differences, including the use of ‘Tensor’ instead of ‘torch’.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch\n\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")  \nprint(device)\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad import Device\n\nprint(Device.DEFAULT)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n        self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n        self.l3 = nn.Linear(1600, 10)\n\n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.l1(x)), (2,2))\n        x = F.max_pool2d(F.relu(self.l2(x)), (2,2))\n        return self.l3(F.dropout(x.flatten(1), 0.5, self.training))\n\n\n\n\n\n\n\nfrom tinygrad import Tensor, nn\n\nclass Model:\n  def __init__(self):\n    self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n    self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n    self.l3 = nn.Linear(1600, 10)\n\n  def __call__(self, x:Tensor) -&gt; Tensor:\n    x = self.l1(x).relu().max_pool2d((2,2))\n    x = self.l2(x).relu().max_pool2d((2,2))\n    return self.l3(x.flatten(1).dropout(0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\n\n# Convert tinygrad Tensors to PyTorch tensors \nX_train = torch.from_numpy(X_train.numpy()).float().reshape(-1, 1, 28, 28) \nY_train = torch.from_numpy(Y_train.numpy()).long()\nX_test = torch.from_numpy(X_test.numpy()).float().reshape(-1, 1, 28, 28) \nY_test = torch.from_numpy(Y_test.numpy()).long()\n\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).float().mean()\nprint(acc.item())  \n\n\n\n\n\n\n\nmodel = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).mean()\nprint(acc.item()) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noptimizer = torch.optim.Adam(model.parameters())\nbatch_size = 128\ndef training_step():\n    model.train()  # enables dropout\n    samples = torch.randint(high=X_train.shape[0], size=(batch_size,))\n    X, Y = X_train[samples], Y_train[samples]\n    optimizer.zero_grad()\n    outputs = model(X)\n    loss = F.nll_loss(F.log_softmax(outputs, dim=1), Y)\n    loss.backward()\n    optimizer.step()\n    return loss\ntrain_losses = []\ntest_losses = []\n\ntrain_accuracies = []\ntest_accuracies = []\n\nfor step in range(7000):\n    # Calculate train loss\n    loss = training_step()  \n    train_losses.append(loss.item())\n    \n    if step % 100 == 0:\n        model.eval()  # Disables dropout for evaluation\n        \n        with torch.no_grad():\n            # Calculate train accuracy\n            train_outputs = model(X_train)\n            train_acc = (train_outputs.argmax(dim=1) == Y_train).float().mean().item()\n            train_accuracies.append(train_acc)\n\n            # Calculate test accuracy\n            test_outputs = model(X_test)\n            test_acc = (test_outputs.argmax(dim=1) == Y_test).float().mean().item()\n            test_accuracies.append(test_acc)\n\n            # Calculate test loss\n            test_loss = F.nll_loss(F.log_softmax(model(X_test), dim=1), Y_test).item()\n            test_losses.append(test_loss)\n\n        print(f\"step {step:4d}, loss {loss.item():.2f}, train acc {train_acc*100.:.2f}%, test acc {test_acc*100.:.2f}%\")\n        model.train()  # Re-enables dropout for training\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(len(train_losses)), train_losses, label='Train Loss')\nplt.plot(range(0, len(test_losses) * 100, 100), test_losses, label='Test Loss') # every 100 steps\nplt.title('PyTorch: Train and Test Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(0, len(train_accuracies) * 100, 100), train_accuracies, label='Train Accuracy')\nplt.plot(range(0, len(test_accuracies) * 100, 100), test_accuracies, label='Test Accuracy')\nplt.title('PyTorch: Train and Test Accuracy')\nplt.xlabel('Steps')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\npytorch_weights = {\n    'l1.weight': model.l1.weight.detach().numpy(),\n    'l1.bias': model.l1.bias.detach().numpy(),\n    'l2.weight': model.l2.weight.detach().numpy(),\n    'l2.bias': model.l2.bias.detach().numpy(),\n    'l3.weight': model.l3.weight.detach().numpy(),\n    'l3.bias': model.l3.bias.detach().numpy()\n}\nnp.save('pytorch_weights.npy', pytorch_weights)\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\nloaded_weights = np.load('pytorch_weights.npy', allow_pickle=True).item()\n\nmodel.l1.weight = Tensor(loaded_weights['l1.weight'])\nmodel.l1.bias = Tensor(loaded_weights['l1.bias'])\nmodel.l2.weight = Tensor(loaded_weights['l2.weight'])\nmodel.l2.bias = Tensor(loaded_weights['l2.bias'])\nmodel.l3.weight = Tensor(loaded_weights['l3.weight'])\nmodel.l3.bias = Tensor(loaded_weights['l3.bias'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\nmodel.eval()\nwith torch.no_grad():\n    pytorch_probs = F.softmax(model(test_image), dim=1).numpy()\nprint(\"PyTorch probabilities:\", pytorch_probs)\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\ntinygrad_probs = model(test_image).softmax().numpy()\nprint(\"tinygrad probabilities:\", tinygrad_probs)"
  },
  {
    "objectID": "blog/posts/1_from_pytorch_to_tinygrad/2_from_tinygrad_to_pytorch.html",
    "href": "blog/posts/1_from_pytorch_to_tinygrad/2_from_tinygrad_to_pytorch.html",
    "title": "From tinygrad to PyTorch",
    "section": "",
    "text": "This is the second post of my series documenting my explorations with comparing PyTorch and tinygrad. Like the first post, I will be using the simple model from tinygrad’s MNIST tutorial and a PyTorch version of the model that I wrote. The experiment in this post does the previous experiment the other way around (training the tinygrad model and transferring the resulting weights to the PyTorch model to see if the two models produce the same probabilities).\nI have the models in two files, ‘tinygrad_MNIST_start.ipynb’ and ‘pytorch_MNIST_end.ipynb’. Below is a side-by-side comparison of the tinygrad and PyTorch code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad import Device\n\nprint(Device.DEFAULT)\n\n\n\n\n\n\n\n\n\n\nimport torch\n\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")  \nprint(device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad import Tensor, nn\n\nclass Model:\n  def __init__(self):\n    self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n    self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n    self.l3 = nn.Linear(1600, 10)\n\n  def __call__(self, x:Tensor) -&gt; Tensor:\n    x = self.l1(x).relu().max_pool2d((2,2))\n    x = self.l2(x).relu().max_pool2d((2,2))\n    return self.l3(x.flatten(1).dropout(0.5))\n\n\n\n\n\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))\n        self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))\n        self.l3 = nn.Linear(1600, 10)\n\n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.l1(x)), (2,2))\n        x = F.max_pool2d(F.relu(self.l2(x)), (2,2))\n        return self.l3(F.dropout(x.flatten(1), 0.5, self.training))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\nfrom tinygrad.nn.datasets import mnist\n\nX_train, Y_train, X_test, Y_test = mnist()\n\n# Convert tinygrad Tensors to PyTorch tensors \nX_train = torch.from_numpy(X_train.numpy()).float().reshape(-1, 1, 28, 28) \nY_train = torch.from_numpy(Y_train.numpy()).long()\nX_test = torch.from_numpy(X_test.numpy()).float().reshape(-1, 1, 28, 28) \nY_test = torch.from_numpy(Y_test.numpy()).long()\n\nprint(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel = Model()\nacc = (model(X_test).argmax(axis=1) == Y_test).mean()\nprint(acc.item())   \n\n\n\n\n\n\n\nmodel = Model().to(device)\nX_train = X_train.to(device)\nY_train = Y_train.to(device)\nX_test = X_test.to(device)\nY_test = Y_test.to(device)\n\nacc = (model(X_test).argmax(axis=1) == Y_test).float().mean()\nprint(acc.item())  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noptim = nn.optim.Adam(nn.state.get_parameters(model))\nbatch_size = 128\ndef step():\n    Tensor.training = True  # makes dropout work\n    samples = Tensor.randint(batch_size, high=X_train.shape[0])\n    X, Y = X_train[samples], Y_train[samples]\n    optim.zero_grad()\n    loss = model(X).sparse_categorical_crossentropy(Y).backward()\n    optim.step()\n    return loss\nfrom tinygrad import TinyJit\n\njit_step = TinyJit(step)\ntrain_losses = []\ntest_losses = []\n\ntrain_accuracies = []\ntest_accuracies = []\n\nfor step in range(7000):\n    # Calculate train loss\n    loss = jit_step()\n    train_losses.append(loss.item())\n\n    if step%100 == 0:\n        Tensor.training = False     # Disables dropout for evaluation\n\n        # Calculate train accuracy\n        #train_outputs = model(X_train)\n        #train_acc = (train_outputs.argmax(axis=1) == Y_train).mean().item()\n        #train_accuracies.append(train_acc)\n\n        # Calculate test accuracy\n        test_outputs = model(X_test)\n        test_acc = (test_outputs.argmax(axis=1) == Y_test).mean().item()\n        test_accuracies.append(test_acc)\n\n        # Calculate test loss\n        test_loss = test_outputs.sparse_categorical_crossentropy(Y_test).mean().item()\n        test_losses.append(test_loss)\n\n        #print(f\"step {step:4d}, loss {loss.item():.2f}, train acc {train_acc*100.:.2f}%, test acc {test_acc*100.:.2f}%\")\n        print(f\"step {step:4d}, loss {loss.item():.2f}, test acc {test_acc*100.:.2f}%\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(len(train_losses)), train_losses, label='Train Loss')\nplt.plot(range(0, len(test_losses) * 100, 100), test_losses, label='Test Loss') # every 100 steps\nplt.title('Tinygrad: Train and Test Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 5))\n#plt.plot(range(0, len(train_accuracies) * 100, 100), train_accuracies, label='Train Accuracy')\nplt.plot(range(0, len(test_accuracies) * 100, 100), test_accuracies, label='Test Accuracy')\nplt.title('Tinygrad: Train and Test Accuracy')\nplt.xlabel('Steps')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\ntinygrad_weights = {\n    'l1.weight': model.l1.weight.numpy(),\n    'l1.bias': model.l1.bias.numpy(),\n    'l2.weight': model.l2.weight.numpy(),\n    'l2.bias': model.l2.bias.numpy(),\n    'l3.weight': model.l3.weight.numpy(),\n    'l3.bias': model.l3.bias.numpy()\n}\nnp.save('tinygrad_weights.npy', tinygrad_weights)\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\nloaded_weights = np.load('tinygrad_weights.npy', allow_pickle=True).item()\n\nmodel.l1.weight.data = torch.tensor(loaded_weights['l1.weight']).to(device)\nmodel.l1.bias.data = torch.tensor(loaded_weights['l1.bias']).to(device)\nmodel.l2.weight.data = torch.tensor(loaded_weights['l2.weight']).to(device)\nmodel.l2.bias.data = torch.tensor(loaded_weights['l2.bias']).to(device)\nmodel.l3.weight.data = torch.tensor(loaded_weights['l3.weight']).to(device)\nmodel.l3.bias.data = torch.tensor(loaded_weights['l3.bias']).to(device)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\ntinygrad_probs = model(test_image).softmax().numpy()\nprint(\"tinygrad probabilities:\", tinygrad_probs)\n\n\n\n\n\n\n\ntest_image = X_test[0:1]\nmodel.eval()\nwith torch.no_grad():\n    pytorch_probs = F.softmax(model(test_image), dim=1).cpu().numpy()\nprint(\"PyTorch probabilities:\", pytorch_probs)"
  }
]