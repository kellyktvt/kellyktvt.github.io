---
title: From PyTorch to tinygrad
description: Training a PyTorch MNIST model and transferring the weights to tinygrad.
author: Kelly To
date: '2024-09-06'
categories: [PyTorch, tinygrad, MNIST]
css: columns.css
---

::::: {.multiple}

-----

:::: {.columns .column-screen-inset}
::: column
This post is the first of what will be a series documenting my explorations with comparing PyTorch and tinygrad, two deep learning frameworks. For the first couple of posts, I will be using the simple model from [tinygrad's MNIST tutorial](https://docs.tinygrad.org/mnist/). I wrote a PyTorch version of the code to compare the two frameworks. The experiment in this post involves training the PyTorch model and transferring the resulting weights to the tinygrad model to see if the two models produce the same probabilities.

I have the models in two files, 'pytorch_MNIST_start.ipynb' and 'tinygrad_MNIST_end.ipynb'. Below is a side-by-side comparison of the PyTorch and tinygrad code. tinygrad's API is quite similar to PyTorch's API, but there are some notable differences, including the use of 'Tensor' instead of 'torch'.
:::
::::

<!---------- Device configuration ---------->

:::: {.columns .column-screen-inset}
::: column
## Device configuration
:::
::::

:::: {.columns .column-screen-inset}
::: column
###### The default device for PyTorch is CPU, but since I am using an M3 Max Macbook Pro, I switched it to MPS for acceleration.
#### PyTorch
```python
import torch

if torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cpu")  
print(device)
```
###### mps
:::
::: column
###### For tinygrad, the default device is based on your system. In this case, it is METAL.
#### tinygrad
```python
from tinygrad import Device

print(Device.DEFAULT)
```
###### METAL
:::
::::

<!---------- Model ---------->

:::: {.columns .column-screen-inset}
::: column
## Model
###### The model is a convolutional neural network with two convolutional layers and a linear layer. The convolutional layers have 32 and 64 filters, respectively. The linear layer has 10 output units, one for each digit. 
:::
::::

:::: {.columns .column-screen-inset}
::: column
###### The PyTorch model inherits from nn.Module, which is a base class in PyTorch for creating models. It calls a forward method to apply the model to the input.
#### PyTorch
```python
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))
        self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))
        self.l3 = nn.Linear(1600, 10)

    def forward(self, x):
        x = F.max_pool2d(F.relu(self.l1(x)), (2,2))
        x = F.max_pool2d(F.relu(self.l2(x)), (2,2))
        return self.l3(F.dropout(x.flatten(1), 0.5, self.training))
```
:::
::: column
###### The tinygrad model is a simple class. Instead of having a forward method, it instead uses a __call__ method that functions like PyTorch's forward method.
#### tinygrad
```python
from tinygrad import Tensor, nn

class Model:
  def __init__(self):
    self.l1 = nn.Conv2d(1, 32, kernel_size=(3,3))
    self.l2 = nn.Conv2d(32, 64, kernel_size=(3,3))
    self.l3 = nn.Linear(1600, 10)

  def __call__(self, x:Tensor) -> Tensor:
    x = self.l1(x).relu().max_pool2d((2,2))
    x = self.l2(x).relu().max_pool2d((2,2))
    return self.l3(x.flatten(1).dropout(0.5))
```
:::
::::

<!---------- Get the MNIST dataset ---------->

:::: {.columns .column-screen-inset}
::: column
## Get the MNIST dataset
###### To get the MNIST dataset, I used tinygrad's mnist function for both models to keep things simple. I did have to convert the tinygrad tensors that were returned by the function into PyTorch tensors to use them in the PyTorch model. 
:::
::::

:::: {.columns .column-screen-inset}
::: column
#### PyTorch
```python
from tinygrad.nn.datasets import mnist

X_train, Y_train, X_test, Y_test = mnist()

# Convert tinygrad Tensors to PyTorch tensors 
X_train = torch.from_numpy(X_train.numpy()).float().reshape(-1, 1, 28, 28).to(device) 
Y_train = torch.from_numpy(Y_train.numpy()).long().to(device)
X_test = torch.from_numpy(X_test.numpy()).float().reshape(-1, 1, 28, 28).to(device) 
Y_test = torch.from_numpy(Y_test.numpy()).long().to(device)

print(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)
```
###### torch.Size([60000, 1, 28, 28]) torch.float32 torch.Size([60000]) torch.int64
:::
::: column
#### tinygrad
```python
from tinygrad.nn.datasets import mnist

X_train, Y_train, X_test, Y_test = mnist()
print(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)
```
###### (60000, 1, 28, 28) dtypes.uchar (60000,) dtypes.uchar
:::
::::

<!---------- Use the model ---------->

:::: {.columns .column-screen-inset}
::: column
## Use the model
###### The models are created and evaluated in the same way. 
:::
::::

:::: {.columns .column-screen-inset}
::: column
#### PyTorch
```python
model = Model().to(device)
acc = (model(X_test).argmax(axis=1) == Y_test).float().mean()
print(acc.item())  
```
###### 0.1185000017285347
:::
::: column
#### tinygrad
```python
model = Model()
acc = (model(X_test).argmax(axis=1) == Y_test).mean()
print(acc.item()) 
```
###### 0.1193000003695488
:::
::::

:::: {.columns .column-screen-inset}
::: column
#### PyTorch
###### The model is trained using the Adam optimizer and the negative log likelihood loss function. It is trained for 7000 steps and is evaluated on the test set after every 100 steps. 
### Train the model
###### It took 50.4 seconds to train the model using PyTorch. 
```python
optim = torch.optim.Adam(model.parameters())
batch_size = 128
def training_step():
    model.train()  # enables dropout
    samples = torch.randint(high=X_train.shape[0], size=(batch_size,))
    X, Y = X_train[samples], Y_train[samples]
    optim.zero_grad()
    loss = F.nll_loss(F.log_softmax(model(X), dim=1), Y)
    loss.backward()
    optim.step()
    return loss
```
```python
train_losses = []
test_losses = []

train_accuracies = []
test_accuracies = []

for step in range(7000):
    # Calculate train loss
    loss = training_step()  
    train_losses.append(loss.item())
    
    if step % 100 == 0:
        model.eval()  # Disables dropout for evaluation
        
        with torch.no_grad():
            # Calculate train accuracy
            train_outputs = model(X_train)
            train_acc = (train_outputs.argmax(dim=1) == Y_train).float().mean().item()
            train_accuracies.append(train_acc)

            # Calculate test accuracy
            test_outputs = model(X_test)
            test_acc = (test_outputs.argmax(dim=1) == Y_test).float().mean().item()
            test_accuracies.append(test_acc)

            # Calculate test loss
            test_loss = F.nll_loss(F.log_softmax(model(X_test), dim=1), Y_test).item()
            test_losses.append(test_loss)

        print(f"step {step:4d}, loss {loss.item():.2f}, train acc {train_acc*100.:.2f}%, test acc {test_acc*100.:.2f}%")
        model.train()  # Re-enables dropout for training
```
###### step    0, loss 12.97, train acc 51.29%, test acc 51.78%
###### step  100, loss 0.19, train acc 94.57%, test acc 94.97%
###### step  200, loss 0.31, train acc 96.34%, test acc 96.55%
###### step  300, loss 0.20, train acc 96.83%, test acc 97.15%
###### ...
###### step 6600, loss 0.02, train acc 99.41%, test acc 98.82%
###### step 6700, loss 0.04, train acc 99.44%, test acc 98.93%
###### step 6800, loss 0.09, train acc 99.45%, test acc 98.87%
###### step 6900, loss 0.05, train acc 99.49%, test acc 98.89%
:::
::::

:::: {.columns .column-screen-inset}
::: column
### Plot the loss
```python
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(range(len(train_losses)), train_losses, label='Train Loss')
plt.plot(range(0, len(test_losses) * 100, 100), test_losses, label='Test Loss') # every 100 steps
plt.title('PyTorch: Train and Test Loss')
plt.xlabel('Steps')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()
```
![](../images/1_from_pytorch_to_tinygrad/pytorch_loss.png)
:::
::: column
### Plot the accuracy
```python
plt.figure(figsize=(10, 5))
plt.plot(range(0, len(train_accuracies) * 100, 100), train_accuracies, label='Train Accuracy')
plt.plot(range(0, len(test_accuracies) * 100, 100), test_accuracies, label='Test Accuracy')
plt.title('PyTorch: Train and Test Accuracy')
plt.xlabel('Steps')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()
```
![](../images/1_from_pytorch_to_tinygrad/pytorch_accuracy.png)
:::
::::

:::: {.columns .column-screen-inset}
::: column
### Save the weights
###### To use the weights from the PyTorch model in the tinygrad model, I saved the weights as a NumPy array.
```python
import numpy as np

pytorch_weights = {
    'l1.weight': model.l1.weight.detach().cpu().numpy(),
    'l1.bias': model.l1.bias.detach().cpu().numpy(),
    'l2.weight': model.l2.weight.detach().cpu().numpy(),
    'l2.bias': model.l2.bias.detach().cpu().numpy(),
    'l3.weight': model.l3.weight.detach().cpu().numpy(),
    'l3.bias': model.l3.bias.detach().cpu().numpy()
}
np.save('pytorch_weights.npy', pytorch_weights)
```
:::
::::

:::: {.columns .column-screen-inset}
::: column
#### tinygrad
### Use the PyTorch weights
###### Then, in my tinygrad model, I loaded the PyTorch weights and set them as the weights of the model. 
```python
import numpy as np

loaded_weights = np.load('pytorch_weights.npy', allow_pickle=True).item()

model.l1.weight = Tensor(loaded_weights['l1.weight'])
model.l1.bias = Tensor(loaded_weights['l1.bias'])
model.l2.weight = Tensor(loaded_weights['l2.weight'])
model.l2.bias = Tensor(loaded_weights['l2.bias'])
model.l3.weight = Tensor(loaded_weights['l3.weight'])
model.l3.bias = Tensor(loaded_weights['l3.bias'])
```
:::
::::

<!---------- Final Probabilities ---------->

:::: {.columns .column-screen-inset}
::: column
## Final Probabilities
###### When comparing the probabilities produced by both models, we can see that they are very similar with negligible variations that can be attributed to floating-point precision differences.
:::
::::

:::: {.columns .column-screen-inset}
::: column
#### PyTorch
```python
test_image = X_test[0:1]
model.eval()
with torch.no_grad():
    pytorch_probs = F.softmax(model(test_image), dim=1).cpu().numpy()
print("PyTorch probabilities:", pytorch_probs)
```
###### PyTorch probabilities: [[2.0321661e-12 1.6674762e-14 4.3643529e-09 3.6196235e-10 1.4885739e-17 1.5148360e-13 5.1869714e-20 1.0000000e+00 1.2723442e-13 8.1969204e-10]]
:::
::: column
#### tinygrad
```python
test_image = X_test[0:1]
tinygrad_probs = model(test_image).softmax().numpy()
print("tinygrad probabilities:", tinygrad_probs)
```
###### tinygrad probabilities: [[2.0321613e-12 1.6674740e-14 4.3643462e-09 3.6196185e-10 1.4885673e-17 1.5148325e-13 5.1869795e-20 1.0000000e+00 1.2723462e-13 8.1969020e-10]]
:::
::::

:::::
